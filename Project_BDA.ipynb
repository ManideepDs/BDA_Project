{"cells":[{"cell_type":"markdown","source":["#KK-Box's Music Recommendation System\n>___Team members___ <br />\n*Anushi Doshi <br />\n*Mahesh kumar Badam venkata <br />\n*Manideep Kannaiah <br />\n*Vidushi Mishra <br />"],"metadata":{}},{"cell_type":"code","source":["#imports\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row ,functions\nfrom pyspark.sql.types import StringType, DateType\nfrom pyspark.sql.dataframe import DataFrame\nfrom pyspark.ml import feature, Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nimport numpy as np\nimport pandas as pd\n\n#Setting up spark session and spark context\nspark = SparkSession.builder.getOrCreate()\nsc = spark.sparkContext"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# read only cell\n\nimport os\n\n# get the databricks runtime version\ndb_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n\n# Define a function to read the data file.  The full path data file name is constructed\n# by checking runtime environment variables to determine if the runtime environment is \n# databricks, or a student's personal computer.  The full path file name is then\n# constructed based on the runtime env.\n# \n# Params\n#   data_file_name: The base name of the data file to load\n# \n# Returns the full path file name based on the runtime env\n#\ndef get_training_filename(data_file_name):    \n    # if the databricks env var exists\n    if db_env != None:\n        # build the full path file name assuming data brick env\n        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n    # else the data is assumed to be in the same dir as this notebook\n    else:\n        # Assume the student is running on their own computer and load the data\n        # file from the same dir as this notebook\n        full_path_name = data_file_name\n    \n    # return the full path file name to the caller\n    return full_path_name\n\n#Function defining the shape of spark dataframe\ndef spark_df_shape(self):\n    return (self.count(), len(self.columns))\n  \n#Plug the function into pyspark\nDataFrame.shape = spark_df_shape"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#Importing Data \nmembers = spark.read.csv(get_training_filename('members.csv'),header='true',inferSchema='true')\nsongs = spark.read.csv(get_training_filename('songs.csv'),header='true',inferSchema='true')\nsongs_extra_info = spark.read.csv(get_training_filename('song_extra_info.csv'),header='true',inferSchema='true')\nlogs = spark.read.csv(get_training_filename('train.csv'),header='true',inferSchema='true')"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["##Basic Data Cleaning and Analysis"],"metadata":{}},{"cell_type":"code","source":["print(\"Data Dimensions:\")\nprint(\"members:\" , members.shape())\nprint(\"songs:\" , songs.shape())\nprint(\"songs_extra_info:\" , songs_extra_info.shape())\nprint(\"logs:\" , logs.shape())"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["members.show(5)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["songs.show(5)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["songs_extra_info.show(5)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["logs.show(5)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#Merfing songs and song_extra_info \nsongs = songs.join(songs_extra_info,\n                   songs.song_id == songs_extra_info.song_id,'left_outer')\\\n  .drop(songs_extra_info.song_id)\\\n  .withColumn('language',songs.language.cast(StringType()))\n\n#Adjusting data types\nmembers = members.withColumn('registrationDate', functions.unix_timestamp(members.registration_init_time.cast(StringType()), 'yyyyMMdd').cast('timestamp'))\\\n                               .withColumn('expirationDate', functions.unix_timestamp(members.expiration_date.cast(StringType()),'yyyyMMdd').cast('timestamp'))\\\n                               .withColumn('Age',members.bd)\\\n                               .drop('registration_init_time','expiration_date','bd')\n                               \n\n#Displaying the data\nsongs.show(5)\nmembers.show(5)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["##Data Visualization"],"metadata":{}},{"cell_type":"code","source":["#display(songs.select('language').orderBy('language').groupBy('language').agg(functions.count('language')).dropna())\n#display(songs.select('genre_ids').groupBy('genre_ids').agg(functions.count('genre_ids')))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["#Model Building"],"metadata":{}},{"cell_type":"markdown","source":["___DataSet Preparation___"],"metadata":{}},{"cell_type":"code","source":["#combining data\nlogs = logs.join(members,members.msno == logs.msno, 'inner').drop(members.msno)\nlogs =logs.join(songs, logs.song_id == songs.song_id, 'inner').drop(songs.song_id)\n\n#String Indexer for msno and Song_id\nuserIdStringIndexer = feature.StringIndexer().setInputCol('msno').setOutputCol('userId')\nsongIdStringIndexer = feature.StringIndexer().setInputCol('song_id').setOutputCol('songId')\n\n#Data Cleaning PipeLines for StringIndexers\ndcPipeline = Pipeline(stages = [userIdStringIndexer,songIdStringIndexer])\n\n#Fit the pipeline and tranforming logsDF\nlogsDf = dcPipeline.fit(logs).transform(logs).drop('msno','song_id')"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#logsDF = logsDf.drop('msno','song_id')\nlogsDf.show()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["##Logistic Regression"],"metadata":{}},{"cell_type":"code","source":["encodeCol = ['source_system_tab','source_screen_name','source_type','city','gender','registered_via','language','genre_ids']\n\n#String Indexers for all categorical variables\npipeline_stages=[]\nfor attr in encodeCol:\n  si = feature.StringIndexer().setInputCol(attr).setOutputCol(attr+'_vec').setHandleInvalid('skip')\n  pipeline_stages.append(si)\n\n#One hot encoding of above String indexers\nohe = feature.OneHotEncoderEstimator().setInputCols([attr+'_vec' for attr in encodeCol]).setOutputCols([attr+'_indexed' for attr in encodeCol])\n\n#feature assembler\nfeatures_indexed = [attr+'_indexed' for attr in encodeCol]\nfeatures_indexed.extend(['Age','song_length'])\nfeature_assembler = feature.VectorAssembler().setInputCols(features_indexed).setOutputCol('features')\n\n#Appending one hot encoder and feature assembler pipeline stages\npipeline_stages.append(ohe)\npipeline_stages.append(feature_assembler)\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["Logit = LogisticRegression().setFeaturesCol('features').setLabelCol('target')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["pipeline_stages.append(Logit)\nEncode_pipeline = Pipeline().setStages(pipeline_stages)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["fit = Encode_pipeline.fit(logsDf)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["fit.stages[-1].summary.areaUnderROC"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[117]: 0.6595261529944415</div>"]}}],"execution_count":23}],"metadata":{"name":"Project_BDA","notebookId":652416647627847},"nbformat":4,"nbformat_minor":0}
